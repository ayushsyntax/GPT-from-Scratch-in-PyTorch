
---

# ðŸ§  GPT-Mini
*A compact GPT-style language model built from scratch, trained on Shakespeare.*  
Learns character-level patterns, style, and rhythm from Shakespeareâ€™s works.  
Fully implemented in PyTorch with a decoder-only Transformer architecture.

---

### ðŸŽ¬ Demo

[GPT-Mini Demo](https://youtu.be/FSzQwOj5jyA)  

*Click to watch GPT-Mini generate Shakespearean text in real time.*

---

## ðŸ“– Overview

**GPT-Mini** is a decoder-only Transformer implemented in **PyTorch**, trained on the complete works of Shakespeare (~1.1M characters).  

It learns **character-level language modeling**, capturing voice, structure, and rhythm from Shakespeareâ€™s plays and poetry.  

---

## âš¡ Model Workflow


```

Prompt â†’ Tokenize â†’ Embed â†’ [Decoder Ã—6] â†’ Linear â†’ Softmax â†’ Next Character

```

### ðŸ§© Components

1. **Tokenizer**  
   - Character-level: each character â†’ unique token  
   - No subword or BPE tokenization  

2. **Embeddings**  
   - Token embedding + **learned positional embeddings** (GPT-style)  

3. **Decoder Block** (Ã—6)  
   - **Pre-LayerNorm** â†’ Causal Self-Attention â†’ Residual  
   - **Pre-LayerNorm** â†’ Feedforward (4Ã— width, GELU) â†’ Residual  

4. **Output**  
   - Linear projection tied to token embeddings  
   - Softmax for next-character probabilities  

5. **Generation**  
   - Autoregressive, supports **temperature** and **top-k sampling**  

---

Input Text: "The king said"
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Char Tokenizer       â”‚ â†’ [56, 4, 32, 17, 8, 11, 52, 5, 1, 20]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token + Learned Position Embeddings â”‚ â†’ Shape: [seq_len, 128]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Decoder Block (Ã—6)         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚ â”‚ LayerNorm    â”‚                  â”‚ â† Pre-LN (GPT-2 style)
â”‚ â”‚ Causal       â”‚                  â”‚
â”‚ â”‚ Self-Attention (4 heads)        â”‚ â†’ Masked: future tokens hidden
â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚        â–¼                          â”‚
â”‚     Residual (+)                  â”‚
â”‚        â–¼                          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚ â”‚ LayerNorm    â”‚                  â”‚ â† Pre-LN
â”‚ â”‚ MLP (128â†’512â†’128)               â”‚ â†’ GELU, Dropout
â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚        â–¼                          â”‚
â”‚     Residual (+)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Final LayerNorm       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LM Head (Linear 128â†’65)â”‚ â†’ Weight tied to token embeddings
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
Autoregressive Generation Loop:
1. Predict next character
2. Append to input
3. Repeat (max 256 chars)
        â”‚
        â–¼
Output: "The king said, and to the next of Marcius..."
```

---

## ðŸ“Š Model Specifications

| Component         | Details |
|------------------|---------|
| Architecture      | Decoder-only Transformer |
| Layers            | 6 |
| Embedding Dim     | 128 |
| Attention Heads   | 4 |
| Context Length    | 256 |
| Vocabulary Size   | 65 (character-level) |
| Parameters        | 1.23M |
| Positional Encoding | Learned embeddings |
| LayerNorm         | Pre-attention & pre-MLP |
| Training Steps    | 10,000 (~on GPU) |

---

## ðŸ† Results

| Metric             | Value |
|-------------------|-------|
| Perplexity         | 3.02 |
| Character Accuracy | 64.9% |
| NLL                | 1.105 |
| BPC                | 1.594 |

> Evaluated on held-out Shakespeare text. Metrics stored in `evaluation_metrics.json`.

---

## ðŸ“‚ Project Structure

```

gpt-mini/
â”œâ”€â”€ src/model/          # attention.py, transformer.py, embeddings.py
â”œâ”€â”€ src/data/           # tokenizer.py, dataloader.py
â”œâ”€â”€ src/train/          # trainer.py, config.py
â”œâ”€â”€ src/utils/          # debug.py, export.py
â”œâ”€â”€ configs/gpt1_char.yaml
â”œâ”€â”€ deploy/app.py       # Gradio: Generate + Evaluate
â”œâ”€â”€ tests/              # Unit tests
â”œâ”€â”€ data/tinyshakespeare.txt
â”œâ”€â”€ train.py
â””â”€â”€ evaluation_metrics.json

````

---

## ðŸš€ How to Run

```bash
python deploy/app.py
````

* Type a prompt (e.g., `"To be or not to"`)
* Generates text character-by-character in Shakespearean style

---

## ðŸ“š References

* **Transformer architecture**: Vaswani et al., *Attention Is All You Need* (2017)
* **Positional embeddings in GPT**: Learned embeddings, GPT-2 style
* Educational guidance: [Karpathy, â€œLetâ€™s build GPT from scratchâ€](https://youtu.be/kCc8FmEb1nY)

> All code and implementation are original, reflecting the design and behavior described.

---

## ðŸŒŸ Philosophy

Small. Transparent. Understandable.
GPT-Mini captures **how transformers generate language**, with focus on clarity and understanding.



