# gpt2_bpe.yaml

# Placeholder â€” char-level is used for speed
model:
  vocab_size: 50257
  embed_dim: 768
  num_layers: 12
  num_heads: 12
  max_seq_len: 1024
  dropout: 0.1

train:
  batch_size: 8
  lr: 3e-4
  max_steps: 10000
  eval_interval: 500
